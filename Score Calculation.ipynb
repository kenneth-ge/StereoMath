{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "b02f4306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess everything\n",
    "import json\n",
    "\n",
    "# Specify the path to your JSON file\n",
    "json_file_path = 'combined.json'\n",
    "\n",
    "# Open and read the JSON file\n",
    "with open(json_file_path, 'r') as file:\n",
    "    # Load JSON data from the file\n",
    "    data = json.load(file)\n",
    "\n",
    "latex_list = [item[\"latex\"] for item in data]\n",
    "output_list = [item[\"output\"] for item in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "1615196c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eli5.sklearn import InvertableHashingVectorizer\n",
    "from eli5.sklearn import FeatureUnhasher\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "class DocumentVectorizer:\n",
    "    def __init__(self):\n",
    "        self.internal_vec = HashingVectorizer(tokenizer=self.tokenize)\n",
    "        self.vectorizer = InvertableHashingVectorizer(self.internal_vec)\n",
    "        self.feature_names = None\n",
    "        \n",
    "        # Initialize a dynamic vocabulary and document frequencies\n",
    "        self.vocabulary = set()\n",
    "        self.document_frequencies = {}\n",
    "        self.idf_values = {}\n",
    "        self.term_indices = {}\n",
    "        \n",
    "        self.total_documents = 0\n",
    "\n",
    "    def calculate_idf(self, term, document_frequencies, total_documents):\n",
    "        df = document_frequencies[term]\n",
    "        idf = math.log((total_documents + 1) / (df + 1)) + 1\n",
    "        return idf\n",
    "        \n",
    "    def fit_transform(self, documents):\n",
    "        \"\"\"\n",
    "        Fit the vectorizer on the provided documents and transform them into feature vectors.\n",
    "        \"\"\"\n",
    "        self.total_documents = len(documents)\n",
    "        tf_matrix = self.vectorizer.fit_transform(documents)\n",
    "        \n",
    "        self.feature_names = self.get_feature_names()\n",
    "        \n",
    "        for new_doc in documents:\n",
    "            new_terms = self.tokenize(new_doc)\n",
    "            processed = set()\n",
    "            for new_term in new_terms:\n",
    "                if new_term in processed:\n",
    "                    continue\n",
    "                processed.add(new_term)\n",
    "                if new_term not in self.vocabulary:\n",
    "                    self.vocabulary.add(new_term)\n",
    "                    self.document_frequencies[new_term] = 1\n",
    "                else:\n",
    "                    self.document_frequencies[new_term] += 1\n",
    "        \n",
    "        self.idf_values = {term: self.calculate_idf(term, self.document_frequencies, self.total_documents) for term in self.vocabulary}\n",
    "        \n",
    "        self.term_indices = {}\n",
    "        \n",
    "        for idx, term in enumerate(self.feature_names):\n",
    "            if type(term) is list:\n",
    "                for x in term:\n",
    "                    self.term_indices[x['name']] = idx\n",
    "                \n",
    "        return self.transform_internal(tf_matrix)\n",
    "        \n",
    "    def transform(self, document):\n",
    "        \"\"\"\n",
    "        Transform the provided documents into feature vectors.\n",
    "        \"\"\"\n",
    "        if self.feature_names is None:\n",
    "            raise ValueError(\"Vectorizer has not been fitted. Call fit_transform first.\")\n",
    "\n",
    "        tf_matrix = self.vectorizer.transform(document)\n",
    "        \n",
    "        return self.transform_internal(tf_matrix)\n",
    "\n",
    "    def transform_internal(self, tf_matrix):\n",
    "        tfidf_matrix = tf_matrix.copy()\n",
    "        \n",
    "        for term, idx in self.term_indices.items():\n",
    "            tfidf_matrix[:, idx] = tf_matrix[:, idx] * self.idf_values[term]\n",
    "        \n",
    "        return tfidf_matrix\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        \"\"\"\n",
    "        Get the feature names (terms) used by the vectorizer.\n",
    "        \"\"\"\n",
    "        return self.vectorizer.get_feature_names()\n",
    "    \n",
    "    def tokenize(self, input_str):\n",
    "        symbols = [' ', '!', '@', '#', '$', '%', '^', '&', '*', '(', ')', '-', '_', '+', '=', '[', ']', '{', '}', ';', ':', '\\'', '\\\"', ',', '.', '<', '>', '/', '?', '|']\n",
    "\n",
    "        tokens = []\n",
    "        curr_token = ''\n",
    "\n",
    "        for c in input_str:\n",
    "            if c in symbols:\n",
    "                if curr_token:\n",
    "                    tokens.append(curr_token)\n",
    "                    curr_token = ''\n",
    "                if c != ' ':\n",
    "                    tokens.append(c)\n",
    "            elif c == '\\\\':\n",
    "                if curr_token:\n",
    "                    tokens.append(curr_token)\n",
    "                    curr_token = ''\n",
    "                curr_token += c\n",
    "            else:\n",
    "                curr_token += c\n",
    "\n",
    "        if curr_token:\n",
    "            tokens.append(curr_token)\n",
    "\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2974f095",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "f4726bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix:\n",
      "\n",
      "Vocabulary Mapping:\n",
      "Feature 0: .\n",
      "0.4252064803598164\n",
      "0.3251320334825997\n",
      "0.3105526673072801\n",
      "0.0\n",
      "Feature 1: ?\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.592769307628588\n",
      "Feature 2: and\n",
      "0.0\n",
      "0.0\n",
      "0.4865407641485108\n",
      "0.0\n",
      "Feature 3: document\n",
      "0.4252064803598164\n",
      "0.6502640669651994\n",
      "0.0\n",
      "0.3783569705698032\n",
      "Feature 4: first\n",
      "0.5252145958082508\n",
      "0.0\n",
      "0.0\n",
      "0.4673461307572138\n",
      "Feature 5: is\n",
      "0.34763415945982157\n",
      "0.26581674173343006\n",
      "0.2538971545683301\n",
      "0.30933161538012166\n",
      "Feature 6: one\n",
      "0.0\n",
      "0.0\n",
      "0.4865407641485108\n",
      "0.0\n",
      "Feature 7: second\n",
      "0.0\n",
      "0.509382158560758\n",
      "0.0\n",
      "0.0\n",
      "Feature 8: the\n",
      "0.34763415945982157\n",
      "0.26581674173343006\n",
      "0.2538971545683301\n",
      "0.30933161538012166\n",
      "Feature 9: third\n",
      "0.0\n",
      "0.0\n",
      "0.4865407641485108\n",
      "0.0\n",
      "Feature 10: this\n",
      "0.34763415945982157\n",
      "0.26581674173343006\n",
      "0.2538971545683301\n",
      "0.30933161538012166\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tokenize(input_str):\n",
    "        symbols = [' ', '!', '@', '#', '$', '%', '^', '&', '*', '(', ')', '-', '_', '+', '=', '[', ']', '{', '}', ';', ':', '\\'', '\\\"', ',', '.', '<', '>', '/', '?', '|']\n",
    "\n",
    "        tokens = []\n",
    "        curr_token = ''\n",
    "\n",
    "        for c in input_str:\n",
    "            if c in symbols:\n",
    "                if curr_token:\n",
    "                    tokens.append(curr_token)\n",
    "                    curr_token = ''\n",
    "                if c != ' ':\n",
    "                    tokens.append(c)\n",
    "            elif c == '\\\\':\n",
    "                if curr_token:\n",
    "                    tokens.append(curr_token)\n",
    "                    curr_token = ''\n",
    "                curr_token += c\n",
    "            else:\n",
    "                curr_token += c\n",
    "\n",
    "        if curr_token:\n",
    "            tokens.append(curr_token)\n",
    "\n",
    "        return tokens\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\"\n",
    "]\n",
    "\n",
    "for i in range(len(documents)):\n",
    "    documents[i] = documents[i].lower()\n",
    "\n",
    "# Create the TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenize, use_idf=True, smooth_idf=True, norm='l2', sublinear_tf=False)\n",
    "\n",
    "# Learn the vocabulary and transform the documents\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get the vocabulary mapping\n",
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Display the TF-IDF values and corresponding vocabulary\n",
    "print(\"TF-IDF Matrix:\")\n",
    "\n",
    "print(\"\\nVocabulary Mapping:\")\n",
    "for feature_index, word in enumerate(vocabulary):\n",
    "    print(f\"Feature {feature_index}: {word}\")\n",
    "    for x in tfidf_matrix.toarray():\n",
    "        print(x[feature_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "cdd08322",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix:\n",
      "\n",
      "\n",
      "Vocabulary Mapping:\n",
      "Feature 0: is\n",
      "0.4082482904638631\n",
      "0.3333333333333333\n",
      "0.3779644730092272\n",
      "0.4082482904638631\n",
      "Feature 1: this\n",
      "0.4082482904638631\n",
      "0.3333333333333333\n",
      "0.3779644730092272\n",
      "0.4082482904638631\n",
      "Feature 2: third\n",
      "0.0\n",
      "0.0\n",
      "0.7242898166052814\n",
      "0.0\n",
      "Feature 3: and\n",
      "0.0\n",
      "0.0\n",
      "-0.7242898166052814\n",
      "0.0\n",
      "Feature 4: the\n",
      "-0.4082482904638631\n",
      "-0.3333333333333333\n",
      "-0.3779644730092272\n",
      "-0.4082482904638631\n",
      "Feature 5: document\n",
      "-0.4993462638159245\n",
      "-0.8154290342094731\n",
      "0.0\n",
      "-0.4993462638159245\n",
      "Feature 6: second\n",
      "0.0\n",
      "0.638763577291385\n",
      "0.0\n",
      "0.0\n",
      "Feature 7: .\n",
      "-0.4993462638159245\n",
      "-0.40771451710473655\n",
      "-0.4623048077871099\n",
      "0.0\n",
      "Feature 8: one\n",
      "0.0\n",
      "0.0\n",
      "-0.7242898166052814\n",
      "0.0\n",
      "Feature 9: ?\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "-0.7823224153193689\n",
      "Feature 10: first\n",
      "-0.6167919780914652\n",
      "0.0\n",
      "0.0\n",
      "-0.6167919780914652\n"
     ]
    }
   ],
   "source": [
    "vectorizer = DocumentVectorizer()\n",
    "\n",
    "# Learn the vocabulary and transform the documents\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get the vocabulary mapping\n",
    "vocabulary = vectorizer.get_feature_names()\n",
    "\n",
    "# Display the TF-IDF values and corresponding vocabulary\n",
    "print(\"TF-IDF Matrix:\")\n",
    "print()\n",
    "\n",
    "print(\"\\nVocabulary Mapping:\")\n",
    "feature_num = 0\n",
    "for feature_index, word in enumerate(vocabulary):\n",
    "    if type(word) is not list:\n",
    "        continue\n",
    "    print(f\"Feature {feature_num}: {word[0]['name']}\")\n",
    "    for x in tfidf_matrix.toarray():\n",
    "        print(x[feature_index])\n",
    "    \n",
    "    feature_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "ef0b1361",
   "metadata": {},
   "outputs": [],
   "source": [
    "## remember that cosine similarity is dot product if we use l2 norm!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "9c0f7929",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = DocumentVectorizer()\n",
    "\n",
    "# Learn the vocabulary and transform the documents\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get the vocabulary mapping\n",
    "vocabulary = vectorizer.get_feature_names()\n",
    "\n",
    "# Display the TF-IDF values and corresponding vocabulary\n",
    "print(\"TF-IDF Matrix:\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a358470",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
